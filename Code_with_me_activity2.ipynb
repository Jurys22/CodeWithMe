{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Graphic User Interface or GUI: Gradio!\n",
        "\n",
        "In this notebook you will learn a new tool called Gradio! This tool will allow you to create a chatbot using a portion of the code you already used in the previous activity.\n",
        "\n",
        "Gradio is an open-source Python library designed to simplify the process of creating customizable UI components for machine learning models. It allows data scientists, developers, and researchers to quickly build and share web applications or demos of their machine learning models without extensive knowledge of web development.\n",
        "\n",
        "With Gradio, you can create interfaces for a wide range of applications, including image classification, text generation, and more. It supports various input and output types like text, images, videos, and audio, making it versatile for showcasing different types of machine learning models.\n",
        "\n",
        "One of the key features of Gradio is its ease of use. You can set up a basic interface with just a few lines of code, and it integrates seamlessly with popular machine learning libraries such as TensorFlow, PyTorch, and Hugging Face Transformers. This integration makes it easy to create demos for pre-trained models or models you've developed yourself.\n",
        "\n",
        "Gradio is also designed with sharing and collaboration in mind. Once an interface is created, it can be shared through a link, embedded in a website, or even integrated into Jupyter notebooks, like the one you are using right now. This makes it an excellent tool for educators, researchers, and developers looking to showcase their work or collaborate with others.\n",
        "\n",
        "---\n",
        "\n",
        "Dans ce cahier, vous apprendrez √† utiliser un nouvel outil appel√© Gradio ! Cet outil vous permettra de cr√©er un chatbot en utilisant une partie du code que vous avez d√©j√† utilis√© dans l'activit√© pr√©c√©dente.\n",
        "\n",
        "Gradio est une biblioth√®que Python open-source con√ßue pour simplifier le processus de cr√©ation de composants d'interface utilisateur personnalisables pour les mod√®les d'apprentissage automatique. Elle permet aux scientifiques de donn√©es, d√©veloppeurs et chercheurs de construire rapidement et partager des applications web ou des d√©mos de leurs mod√®les d'apprentissage automatique sans avoir une connaissance approfondie du d√©veloppement web.\n",
        "\n",
        "Avec Gradio, vous pouvez cr√©er des interfaces pour une large gamme d'applications, y compris la classification d'images, la g√©n√©ration de texte, et plus encore. Elle prend en charge divers types d'entr√©es et de sorties comme le texte, les images, les vid√©os et l'audio, ce qui la rend polyvalente pour pr√©senter diff√©rents types de mod√®les d'apprentissage automatique.\n",
        "\n",
        "L'une des caract√©ristiques cl√©s de Gradio est sa facilit√© d'utilisation. Vous pouvez configurer une interface de base avec seulement quelques lignes de code, et elle s'int√®gre parfaitement avec des biblioth√®ques d'apprentissage automatique populaires telles que TensorFlow, PyTorch, et Hugging Face Transformers. Cette int√©gration facilite la cr√©ation de d√©mos pour des mod√®les pr√©-entra√Æn√©s ou des mod√®les que vous avez d√©velopp√©s vous-m√™me.\n",
        "\n",
        "Gradio est √©galement con√ßue avec le partage et la collaboration √† l'esprit. Une fois une interface cr√©√©e, elle peut √™tre partag√©e via un lien, int√©gr√©e dans un site web, ou m√™me int√©gr√©e dans des notebook Jupyter comme celui-l√† que tu es en train d'utiliser. Cela en fait un excellent outil pour les √©ducateurs, les chercheurs et les d√©veloppeurs cherchant √† pr√©senter leur travail ou √† collaborer avec d'autres."
      ],
      "metadata": {
        "id": "6qRok2zjTAdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's explore Gradio\n",
        "\n",
        "In the first cell We will first of all install the package gradio and import it here.\n",
        "\n",
        "Then in the second cell we will create the function that Gradio needs to have as action to execute, in our example here it will be function hello_user.\n",
        "\n",
        "Finally we will launch the interface! To launch the interface, we need only to call it gr.Interface and specify that the input will be a text (our name for example!), and we expect an output as a text and the function to be run\n",
        "\n",
        "Let's try it out!\n",
        "\n",
        "---\n",
        "\n",
        "Dans la premi√®re cellule, nous allons tout d'abord installer le paquet gradio et l'importer ici.\n",
        "\n",
        "Ensuite, dans la deuxi√®me cellule, nous allons cr√©er la fonction dont Gradio a besoin comme action √† ex√©cuter, dans notre exemple ici ce sera la fonction hello_user.\n",
        "\n",
        "Enfin, nous lancerons l'interface ! Pour lancer l'interface, il suffit de l'appeler avec gr.Interface et de sp√©cifier qu'on s'attend √† une entr√©e en tant que texte (peut-etre votre pr√©nom !), une sortie en tant que texte et la fonction √† ex√©cuter.\n",
        "\n",
        "Essayons !"
      ],
      "metadata": {
        "id": "HMoVaniSkfvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "-YfVqSMIkuEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hello_user(input):\n",
        "  return(\"Hey! \" + input + \", Take care! \")"
      ],
      "metadata": {
        "id": "VpgJ9QmIk1zb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(inputs = gr.Text(), outputs = gr.Text(), fn = hello_user).launch()"
      ],
      "metadata": {
        "id": "qGENgHA-k8FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# And now it's time to use our code / C'est le moment d'utiliser notre code !\n",
        "\n",
        "Similarly to the first activity we would need to import some packages and dependencies to make our code work.\n",
        "\n",
        "---\n",
        "\n",
        "De mani√®re similaire √† la premi√®re activit√©, nous devrons importer certains packages et d√©pendances pour que notre code fonctionne."
      ],
      "metadata": {
        "id": "hOyVL-LCt4w9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3VSMQ73te_4"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!pip install torch transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and also our model!\n",
        "\n",
        "---\n",
        "\n",
        "et aussi notre mod√®le !"
      ],
      "metadata": {
        "id": "SaMGcDP5tx3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.float16)\n",
        "\n",
        "model = model.to('cuda:0')"
      ],
      "metadata": {
        "id": "eDNcgME-S8Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some modification of our previous code / Quelques modifications √† faire au code pr√©cedent\n",
        "\n",
        "Since we are going to build our chatbot we need to add 2 new ingredients to our recipe:\n",
        "\n",
        "\n",
        "1.   a way to tell to the model that when it meets some characters it needs to stop - this will be done through the *class StopOnTokens()*\n",
        "2.   a way to let the model have in memory what we have said previously - this will be done inside the function *def predict()*\n",
        "\n",
        "Of course there are some limitations! The model we are using is for the English language and it is not the biggest model in the market, so probably we won't be able to ask it anything we want without encountering some... surprises! (or hallucination!)\n",
        "\n",
        "Similarly to the previous activity, use your - not artificial - intelligence and be responsible and stay curious by playing with it.\n",
        "\n",
        "---\n",
        "\n",
        "Puisque nous allons construire notre chatbot, nous devons ajouter 2 nouveaux ingr√©dients √† notre recette :\n",
        "\n",
        "1. un moyen de dire au mod√®le que, lorsqu'il rencontre certains caract√®res, il doit s'arr√™ter - cela sera r√©alis√© gr√¢ce √† la *class StopOnTokens()*\n",
        "2. un moyen de permettre au mod√®le de se souvenir de ce que nous avons dit pr√©c√©demment - cela sera fait √† l'int√©rieur de la fonction *def predict()*\n",
        "\n",
        "Bien s√ªr, il y a des limites ! Le mod√®le que nous utilisons est pour la langue anglaise et ce n'est pas un grand mod√®le, donc probablement nous ne pourrons pas lui demander tout ce que nous voulons sans rencontrer quelques... surprises ! (ou hallucinations !)\n",
        "\n",
        "De mani√®re similaire √† l'activit√© pr√©c√©dente, utilisez votre intelligence - non artificielle - et soyez responsable et restez curieuses en jouant avec."
      ],
      "metadata": {
        "id": "J3cMX2DjUKtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to make the model stop the text generation / Comment arreter la g√©n√©ration du texte.\n",
        "\n",
        "The class StopOnTokens is designed to be a criterion for stopping the text generation process. This is useful in scenarios where you want the generation to stop if certain conditions are met, for example, when a specific token/word or set of tokens/words is generated.\n",
        "\n",
        "The class defines a list stop_ids containing token IDs (in this case, [29, 0]) that, when generated, should cause text generation to stop.\n",
        "These token IDs would correspond to specific conditions or characters (like end-of-text).\n",
        "\n",
        "It then iterates over each stop_id in stop_ids. For the current set of generated tokens (input_ids), it checks if the last token generated (input_ids[0][-1]) is equal to the stop_id.\n",
        "If any of the stop_ids match the last generated token, the method returns True, signaling that the stopping condition has been met and text generation should stop.\n",
        "If none of the stop_ids match the last generated token, it returns False, indicating that text generation should continue.\n",
        "\n",
        "---\n",
        "\n",
        "La classe StopOnTokens est con√ßue pour √™tre un crit√®re d'arr√™t du processus de g√©n√©ration de texte. Cela est utile dans les sc√©narios o√π vous souhaitez que la g√©n√©ration s'arr√™te si certaines conditions sont remplies, par exemple, lorsqu'un token/mot sp√©cifique ou un ensemble de tokens/mots est g√©n√©r√©.\n",
        "\n",
        "La classe d√©finit une liste stop_ids contenant des IDs de tokens (dans ce cas, [29, 0]) qui, lorsqu'ils sont g√©n√©r√©s, devraient provoquer l'arr√™t de la g√©n√©ration de texte.\n",
        "Ces IDs de token correspondraient √† des conditions ou des caract√®res sp√©cifiques (comme endoftext).\n",
        "\n",
        "Elle it√®re ensuite sur chaque stop_id dans stop_ids. Pour l'ensemble actuel de tokens g√©n√©r√©s (input_ids), elle v√©rifie si le dernier token g√©n√©r√© (input_ids[0][-1]) est √©gal au stop_id.\n",
        "Si l'un des stop_ids correspond au dernier token g√©n√©r√©, la m√©thode retourne True, signalant que la condition d'arr√™t a √©t√© atteinte et que la g√©n√©ration de texte doit s'arr√™ter.\n",
        "Si aucun des stop_ids ne correspond au dernier token g√©n√©r√©, elle retourne False, indiquant que la g√©n√©ration de texte doit continuer."
      ],
      "metadata": {
        "id": "UqfeUWTGVj3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        stop_ids = [29, 0] #< ,  <|endoftext|>\n",
        "        for stop_id in stop_ids:\n",
        "            if input_ids[0][-1] == stop_id:\n",
        "                return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "pXbOafe2t1h3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Function / Fonction predict\n",
        "\n",
        "\n",
        "This code defines a function named *predict* that generates responses for a chatbot based on a given message and the conversation history. The process involves a series of steps to prepare the input, generate a response using a language model, and yield the response incrementally.\n",
        "\n",
        "Let's break it down!\n",
        "\n",
        "# 1 History Transformation\n",
        "\n",
        "*history_transformer_format = history + [[message, \"\"]]*: This line formats the conversation history by appending the new message to the end of the history. Each entry in the history is expected to be a list of two strings: one from the human and the response from the bot. The new message is appended without a corresponding bot response (yet).\n",
        "# 2 the new Stop condition\n",
        "*stop = StopOnTokens()*: This is the implementation of what we have seen before for the stopping criteria!\n",
        "\n",
        "# 3 Formatting Input\n",
        "Messages from the history_transformer_format are concatenated into a single string, messages, formatted with special markers (\\n<human>: and \\n<bot>:) to differentiate between messages from the human and responses from the bot. This will allows the user to talk with the model without writing the roles and so.. Be more natural!\n",
        "\n",
        "# 4 Preparing model input\n",
        "*model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\"):* Tokenizes the concatenated messages, converting them into a numerical format that the model can understand (model_inputs)\n",
        "\n",
        "# 5 Streaming and Generation Setup\n",
        "\n",
        "Initializes a TextIteratorStreamer, an object that manages streaming output tokens from the model. This is used because we are in a chatbot environment so the Streamer stores print-ready text in a queue, to be used by a downstream application as an iterator.\n",
        "\n",
        "generate_kwargs is a dictionary that aggregates arguments for the generation function, including some of the parameters that you already used like max_new_tokens, do_sample, top_p, top_k, temperature, num_beams, and the stopping_criteria.\n",
        "\n",
        "# 6 Generation Execution:\n",
        "\n",
        "A Thread is started targeting the model's generate method with the arguments prepared in generate_kwargs. This asynchronous approach allows the function to yield partial messages as they're being generated without waiting for the entire generation process to complete.\n",
        "\n",
        "#7 Yielding Partial Responses:\n",
        "\n",
        "The function iterates over new tokens from the streamer. As long as the new token is not the special character <, it concatenates these tokens to partial_message and yields the partial message. This allows the function to provide real-time updates of the message being generated, useful for applications that want to show the user that the bot is \"typing\" or gradually revealing its response.\n",
        "\n",
        "----\n",
        "\n",
        "# Fonction Predict / Fonction predict\n",
        "\n",
        "Ce code d√©finit une fonction nomm√©e *predict* qui g√©n√®re des r√©ponses pour un chatbot bas√© sur un message donn√© et l'historique de la conversation. Le processus implique une s√©rie d'√©tapes pour pr√©parer l'entr√©e, g√©n√©rer une r√©ponse en utilisant un mod√®le de langage, et fournir la r√©ponse de mani√®re incr√©mentale.\n",
        "\n",
        "D√©composons cela !\n",
        "\n",
        "# 1 Transformation de l'historique\n",
        "\n",
        "*history_transformer_format = history + [[message, \"\"]]* : Cette ligne formate l'historique de la conversation en ajoutant le nouveau message √† la fin de l'historique. Chaque entr√©e dans l'historique est suppos√©e √™tre une liste de deux cha√Ænes : une de l'humain et la r√©ponse du bot. Le nouveau message est ajout√© sans une r√©ponse correspondante du bot (pour l'instant).\n",
        "# 2 La nouvelle condition d'arr√™t\n",
        "*stop = StopOnTokens()* : C'est la mise en ≈ìuvre de ce que nous avons vu pr√©c√©demment pour les crit√®res d'arr√™t !\n",
        "\n",
        "# 3 Formatage de l'entr√©e\n",
        "Les messages de l'history_transformer_format sont concat√©n√©s en une seule cha√Æne, messages, format√©e avec des marqueurs sp√©ciaux (\\n<human> : et \\n<bot> :) pour diff√©rencier les messages de l'humain et les r√©ponses du bot. Cela permettra √† l'utilisateur de parler avec le mod√®le sans √©crire les r√¥les et donc... √ätre plus naturel !\n",
        "\n",
        "# 4 Pr√©paration de l'entr√©e du mod√®le\n",
        "*model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\") :* Tokenise les messages concat√©n√©s, les convertissant en un format num√©rique que le mod√®le peut comprendre (model_inputs)\n",
        "\n",
        "# 5 Configuration du Streaming et de la G√©n√©ration\n",
        "\n",
        "Initialise un TextIteratorStreamer, un objet qui g√®re le flux de tokens de sortie du mod√®le. Ceci est utilis√© parce que nous sommes dans un environnement de chatbot, donc le Streamer stocke le texte pr√™t √† √™tre imprim√© dans une file d'attente, pour √™tre utilis√© par une application en aval comme un it√©rateur.\n",
        "\n",
        "generate_kwargs est un dictionnaire qui agr√®ge les arguments pour la fonction de g√©n√©ration, incluant certains des param√®tres que vous avez d√©j√† utilis√©s comme max_new_tokens, do_sample, top_p, top_k, temperature, num_beams, et les crit√®res d'arr√™t.\n",
        "\n",
        "# 6 Ex√©cution de la G√©n√©ration :\n",
        "\n",
        "Un Thread est d√©marr√© ciblant la m√©thode generate du mod√®le avec les arguments pr√©par√©s dans generate_kwargs. Cette approche asynchrone permet √† la fonction de fournir des messages partiels au fur et √† mesure de leur g√©n√©ration sans attendre que le processus de g√©n√©ration complet soit termin√©.\n",
        "\n",
        "# 7 Fourniture de R√©ponses Partielles :\n",
        "\n",
        "La fonction it√®re sur les nouveaux tokens du streamer. Tant que le nouveau token n'est pas le caract√®re sp√©cial <, il concat√®ne ces tokens √† partial_message et fournit le message partiel. Cela permet √† la fonction de fournir des mises √† jour en temps r√©el du message en cours de g√©n√©ration, utile pour les applications qui veulent montrer √† l'utilisateur que le bot est \"en train d'√©crire\" ou r√©v√®le progressivement sa r√©ponse."
      ],
      "metadata": {
        "id": "KohW2eRyaph_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise for you! / Petit exercice pour vous !\n",
        "\n",
        "Without looking at the solution üòú, insert inside *generate_kwargs = dict( ... )*  the parameters of the model we have seen previously\n",
        "\n",
        "---\n",
        "\n",
        "Sans regarder la solution plus en bas üòú, mettez les param√®tres que vous avez utilis√©s dans l'autre activit√© dedans *generate_kwargs = dict( ... )*"
      ],
      "metadata": {
        "id": "UHgj22sodx-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(message, history):\n",
        "\n",
        "\n",
        "    history_transformer_format = history + [[message, \"\"]]\n",
        "    stop = StopOnTokens()\n",
        "\n",
        "    messages = \"\".join([\"\".join([\"\\n<human>:\"+item[0], \"\\n<bot>:\"+item[1]])\n",
        "                for item in history_transformer_format])\n",
        "\n",
        "    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\")\n",
        "    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        model_inputs,\n",
        "        streamer=streamer,\n",
        "        stopping_criteria=StoppingCriteriaList([stop])\n",
        "        )\n",
        "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "\n",
        "\n",
        "    partial_message = \"\"\n",
        "    for new_token in streamer:\n",
        "        if new_token != '<':\n",
        "            partial_message += new_token\n",
        "            yield partial_message\n",
        "\n"
      ],
      "metadata": {
        "id": "Y05MdLpIXRDq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution here!\n",
        "\n",
        "I hope you were able to insert the parameters from the previous model, if not it is not a problem here you will know how to do it!\n",
        "\n",
        "---\n",
        "\n",
        "J'esp√®re vous avez r√©ussi ! et si vous avez des doutes... Voil√† la solution !"
      ],
      "metadata": {
        "id": "C-nEtTEIeOxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(message, history):\n",
        "\n",
        "\n",
        "    history_transformer_format = history + [[message, \"\"]]\n",
        "    stop = StopOnTokens()\n",
        "\n",
        "    messages = \"\".join([\"\".join([\"\\n<human>:\"+item[0], \"\\n<bot>:\"+item[1]])\n",
        "                for item in history_transformer_format])\n",
        "\n",
        "    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\")\n",
        "    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        model_inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=80,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        #top_k=1000,\n",
        "        temperature=0.6,\n",
        "        num_beams=1,\n",
        "        stopping_criteria=StoppingCriteriaList([stop])\n",
        "        )\n",
        "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "    partial_message = \"\"\n",
        "    for new_token in streamer:\n",
        "        if new_token != '<':\n",
        "            partial_message += new_token\n",
        "            yield partial_message\n",
        "\n"
      ],
      "metadata": {
        "id": "h2bcCXEWeNkT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# launch the chatbot!\n",
        "\n",
        "it's time to create your chatbot. We will use the command *gr.ChatInterface* - that stands for gradio.ChatInterface that will create a simple interface for us.\n",
        "\n",
        "We will also insert the predict function discussed above, so that when we run the app we will also run the predict function and we will begin our chat.\n",
        "\n",
        "Note: sometimes it might display en error and this might be due to Colab, in case retry! As you see we also put *debug=True* this will help us understand most of the time the source of the error!\n",
        "\n",
        "---\n",
        "\n",
        "il est temps de cr√©er votre chatbot. Nous utiliserons la commande *gr.ChatInterface* - qui repr√©sente gradio.ChatInterface et qui cr√©era une interface simple pour nous.\n",
        "\n",
        "Nous ins√©rerons √©galement la fonction predict discut√©e pr√©c√©demment, de sorte que lorsque nous ex√©cuterons l'application, nous ex√©cuterons √©galement la fonction predict et nous commencerons notre discussion.\n",
        "\n",
        "Note : parfois, cela peut afficher une erreur et cela pourrait √™tre d√ª √† Colab, dans ce cas, r√©essayez ! Comme vous le voyez, nous mettons aussi *debug=True *cela nous aidera la plupart du temps √† comprendre l'origine de l'erreur !"
      ],
      "metadata": {
        "id": "DbLa-vQ1frh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(predict).launch(debug=True)"
      ],
      "metadata": {
        "id": "8eSMFbk9ezeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# what if... I don't want other users to play with my chatbot? / Et si je voudrais d√©cider qui peut jouer avec mon chatbot ?\n",
        "\n",
        "Gradio allows you easily to restrict the access to the people who have the right username and password.\n",
        "\n",
        "After the *debug=True* you can specify a parameter called *auth = ('user', 'admin')* where you can change the words \"user\" and \"admin\" to what you decide being the username and the password to enter the chatbot.\n",
        "\n",
        "You can also add a message to the user to let them know that there is a password and username to enter to let them use the chatbot.\n",
        "\n",
        "----\n",
        "\n",
        "Gradio vous permet facilement de restreindre l'acc√®s aux personnes qui ont le bon nom d'utilisateur et mot de passe.\n",
        "\n",
        "Apr√®s le *debug=True*, vous pouvez sp√©cifier un param√®tre appel√© *auth = ('user', 'admin')* o√π vous pouvez changer les mots \"user\" et \"admin\" par ce que vous d√©cidez d'√™tre le nom d'utilisateur et le mot de passe pour entrer dans le chatbot.\n",
        "\n",
        "Vous pouvez √©galement ajouter un message pour l'utilisateur pour les informer qu'il y a un nom d'utilisateur et un mot de passe √† entrer pour leur permettre d'utiliser le chatbot."
      ],
      "metadata": {
        "id": "KYyuYiD5g5d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(predict).launch(debug=True, auth = ('user','admin'), auth_message= \"Enter your username and password to play with my chatbot!\")"
      ],
      "metadata": {
        "id": "Y6QFfdSHXT8r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}